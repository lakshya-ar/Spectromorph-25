{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946cf93b",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "\n",
    "Objective: \n",
    "To design and implement a Convolutional Neural Network (CNN) to classify hand-drawn emojis from a small labeled dataset. The objective of this exercise is to accurately discriminate between various categories of emoji with a compact and efficient CNN architecture that you will build. \n",
    "\n",
    "Constraints: \n",
    "\n",
    "- No Transfer Learning:\n",
    "You are prohibited from using any pre-trained models (e.g., VGG, ResNet, MobileNet, etc.). All components of the model must be constructed from scratch using conventional deep learning frameworks (e.g., TensorFlow/Keras or PyTorch).\n",
    "\n",
    "- Model Simplicity: \n",
    "Your CNN is limited to a maximum of 3 hidden layers (don't include input and output layers) so you can limit the complexity of the architecture and take an emphasis to build on the core principles of CNNs, e.g., Convolution, pooling and dense layers. \n",
    "\n",
    "- Data Constraints: \n",
    "The dataset is small, it consists of hand-drawn images of emojis with various styles and categories. You must keep in mind the constraints of the data and design your model to appropriately account for this data, including the use of regularization or data augmentation appropriately.\n",
    "\n",
    "For the dataset link: https://drive.google.com/drive/folders/1Uo5WCK3z35z8k4k3gVfHn_N-OoE9rLNt?usp=sharing\n",
    "\n",
    "The following model tries to correctly classsify the hand-drawn emojis using CNNs. The accuracy achieved by the model is roughly around **50%** which is decent considering the size of the dataset. The scope of further improvements remains open and any further improvements will be pushed (documented as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad25a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e2c75",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "1. **Loading the Data** - The dataset contain 18 classes with each classs containing 30 400X400 images. This means the dataset contians a total of 540 examples. The images are loaded with the help of opencv, resized(128X128) and stored in lists. It is quite important to mention that the these are **transparent .png files** therefore the background is changed from transparent to white to ensure no background overlay problem occurs when we train the model, this is followed by grayscaling the images. The dataset is then split into three sets **Training, Validation, Test** using the train_test_split function from scikit learn.\n",
    "2. **Augmentation of the Data** - Since the dataset is quite small, we use the ImageDataGenerator from Keras to augment our data. The following augmentations are performed:\n",
    "    - Rotation\n",
    "    - Horizontal & Vertical Shift\n",
    "    - Skewing\n",
    "    - Zoom\n",
    "    - Flipping\n",
    "    \n",
    "    Lastly all the new pixels that might occur due to the augmentation are filled according to their nearest neighbour, this is to make sure  that the new pixels remain mostly as the background $i.e.$ white. \n",
    "3. **Model Architecture** - <pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
    "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
    "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
    "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16384</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16384</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">294,930</span> │\n",
    "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    "</pre><pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">318,786</span> (1.22 MB)\n",
    "</pre><pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">318,562</span> (1.22 MB)\n",
    "</pre><pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
    "</pre>\n",
    "The model architecture also contains a special class whose purpose is to stop the epochs once 60% validation accuracy is reached. This was implemented since the accuracy was seen to deteriorate after a few more epochs leading to poor performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b62730",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.getcwd()\n",
    "root_dir = os.path.join(project_dir, \"dataset\")\n",
    "size_image = (128, 128)\n",
    "x_set, y_set = [],[]\n",
    "class_names = sorted(os.listdir(root_dir))\n",
    "class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
    "for cls in class_names:\n",
    "    cls_path = os.path.join(root_dir, cls)\n",
    "    for img_file in os.listdir(cls_path):\n",
    "        img_path = os.path.join(cls_path, img_file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        if img.shape[-1] == 4:\n",
    "            alpha_channel = img[:, :, 3]\n",
    "            rgb_channels = img[:, :, :3]\n",
    "            white_background = np.ones_like(rgb_channels, dtype=np.uint8) * 255\n",
    "            alpha_factor = alpha_channel[:, :, np.newaxis] / 255.0\n",
    "            img = rgb_channels * alpha_factor + white_background * (1 - alpha_factor)\n",
    "            img = img.astype(np.uint8)\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, size_image)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        x_set.append(img)\n",
    "        y_set.append(class_to_idx[cls])\n",
    "x_all = np.expand_dims(np.array(x_set), axis=-1)\n",
    "y_all = np.array(y_set)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.1, stratify=y_all, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.3, stratify=y_train, random_state=42)   \n",
    "x_train = np.expand_dims(np.array(x_train), axis=-1)\n",
    "x_val = np.expand_dims(np.array(x_val), axis=-1)\n",
    "x_test = np.expand_dims(np.array(x_test), axis=-1)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043b1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "Augementator = ImageDataGenerator(\n",
    "    rotation_range = 5,\n",
    "    width_shift_range = 0.01,\n",
    "    height_shift_range  = 0.01,\n",
    "    shear_range = 0.01,\n",
    "    zoom_range  = 0.01,\n",
    "    horizontal_flip  = True,\n",
    "    fill_mode = 'nearest',\n",
    ")\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(128,128,1)),\n",
    "    Conv2D(16,kernel_size=4,activation='relu',padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(32,kernel_size=3,activation='relu',padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(64,kernel_size=3,activation='relu',padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dropout(0.3),\n",
    "    Dense(18, activation='softmax'),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4971884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 128, 128, 1) (340,)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.1135 - loss: 8.3974 - val_accuracy: 0.0548 - val_loss: 2.9504\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.4075 - loss: 4.0757 - val_accuracy: 0.0548 - val_loss: 4.9435\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.5200 - loss: 2.9474 - val_accuracy: 0.0548 - val_loss: 7.0416\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.6038 - loss: 1.9284 - val_accuracy: 0.0548 - val_loss: 9.7866\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.6965 - loss: 1.4598 - val_accuracy: 0.0548 - val_loss: 10.7700\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.7431 - loss: 1.6102 - val_accuracy: 0.0548 - val_loss: 11.3566\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.7865 - loss: 1.3068 - val_accuracy: 0.0548 - val_loss: 10.2537\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8175 - loss: 0.9070 - val_accuracy: 0.0548 - val_loss: 11.2466\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8263 - loss: 0.8107 - val_accuracy: 0.0548 - val_loss: 11.1438\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8589 - loss: 0.5846 - val_accuracy: 0.0753 - val_loss: 8.7325\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8909 - loss: 0.4254 - val_accuracy: 0.1096 - val_loss: 9.0242\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8776 - loss: 0.4818 - val_accuracy: 0.1644 - val_loss: 7.4561\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8689 - loss: 0.5126 - val_accuracy: 0.1849 - val_loss: 6.9894\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8875 - loss: 0.6341 - val_accuracy: 0.2397 - val_loss: 5.4321\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9005 - loss: 0.5227 - val_accuracy: 0.3562 - val_loss: 4.1082\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9282 - loss: 0.3835 - val_accuracy: 0.3630 - val_loss: 3.9931\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9145 - loss: 0.2202 - val_accuracy: 0.4247 - val_loss: 3.4358\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9430 - loss: 0.1688 - val_accuracy: 0.5205 - val_loss: 2.6366\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9389 - loss: 0.2550 - val_accuracy: 0.4863 - val_loss: 3.1802\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9521 - loss: 0.1549 - val_accuracy: 0.5548 - val_loss: 2.1855\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9051 - loss: 0.4416 - val_accuracy: 0.5342 - val_loss: 2.8219\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9216 - loss: 0.2181 - val_accuracy: 0.4863 - val_loss: 3.7673\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8883 - loss: 0.4341 - val_accuracy: 0.5822 - val_loss: 2.9442\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8854 - loss: 0.5914 - val_accuracy: 0.5411 - val_loss: 3.0996\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9467 - loss: 0.2481 - val_accuracy: 0.4589 - val_loss: 4.4087\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9447 - loss: 0.1827 - val_accuracy: 0.5685 - val_loss: 3.0744\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9524 - loss: 0.1702 - val_accuracy: 0.5890 - val_loss: 3.4196\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9457 - loss: 0.2230 - val_accuracy: 0.4726 - val_loss: 5.2108\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9452 - loss: 0.1722 - val_accuracy: 0.5685 - val_loss: 3.6743\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9350 - loss: 0.2699 \n",
      "60.27% validation accuracy is reached which is optimal for early stoppage.\n",
      "\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.9351 - loss: 0.2698 - val_accuracy: 0.6027 - val_loss: 3.5132\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5494 - loss: 4.8969\n"
     ]
    }
   ],
   "source": [
    "class Stop(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, target=0.60):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = logs.get(\"val_accuracy\")\n",
    "        if val_acc is not None and val_acc > self.target:\n",
    "            print(f\" \\n{val_acc*100:.2f}% validation accuracy is reached which is optimal for early stoppage.\\n\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "x_train = np.array(x_train).reshape(-1, 128, 128, 1)\n",
    "x_test = np.array(x_test).reshape(-1, 128, 128, 1)\n",
    "x_val = np.array(x_val).reshape(-1,128,128,1)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(np.array(x_train).shape, np.array(y_train).shape)\n",
    "train_generator = Augementator.flow(x_train, y_train, batch_size=16)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks = [Stop(0.60)]\n",
    ")\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
